{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Import Necessary Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Processed Data (v1.0) ---\n",
    "data_path = '../data/raw_data_1.0.csv'\n",
    "\n",
    "print(f\"--- Loading v1.0 data ---\")\n",
    "print(f\"File path: '{data_path}'\")\n",
    "\n",
    "data_path_v2 = '../data/raw_data_2.0.csv'\n",
    "\n",
    "print(f\"--- Loading v2.0 (PCTL_BNT cleaned) data ---\")\n",
    "df_v2 = pd.read_csv(data_path_v2)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"\\nData loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Data file not found at '{data_path}'\")\n",
    "    print(\"Please confirm you have successfully run the processing/process_main_dataset.py and processing/clean_pctl_bnt.py scripts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1. Optimize Data Types ---\n",
    "print(\"Checking for columns that can be converted from float to integer...\")\n",
    "\n",
    "cols_converted = []\n",
    "for col in df.columns:\n",
    "    # Only check float type columns\n",
    "    if pd.api.types.is_float_dtype(df[col]):\n",
    "        # Check if all non-null values can be considered integers\n",
    "        if (df[col].dropna() % 1 == 0).all():\n",
    "            # Use nullable integer type 'Int64' to handle potential NaN values\n",
    "            df[col] = df[col].astype('Int64')\n",
    "            cols_converted.append(col)\n",
    "\n",
    "if cols_converted:\n",
    "    print(f\"\\nSuccessfully converted {len(cols_converted)} columns to 'int64':\")\n",
    "    # For brevity, we only print the first few\n",
    "    print(sorted(cols_converted)[:10], \"...\")\n",
    "else:\n",
    "    print(\"\\nNo columns needed conversion.\")\n",
    "\n",
    "print(\"\\n--- Optimized Data Types ---\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Basic Data Overview ---\n",
    "print(\"--- Basic Data Information ---\")\n",
    "print(f\"Data Shape (Rows, Columns): {df.shape}\\n\")\n",
    "\n",
    "print(\"--- First 20 Rows of Data ---\")\n",
    "display(df.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1. Feature Type Analysis ---\n",
    "import json\n",
    "\n",
    "print(\"--- Feature Type Analysis ---\")\n",
    "\n",
    "# Load expected value ranges\n",
    "expected_ranges_path = '../data/expected_value_range.json'\n",
    "try:\n",
    "    with open(expected_ranges_path, 'r') as f:\n",
    "        expected_ranges = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Expected value range file not found at '{expected_ranges_path}'\")\n",
    "    expected_ranges = {}\n",
    "\n",
    "# Store analysis results\n",
    "feature_analysis = []\n",
    "\n",
    "# Iterate over all columns\n",
    "for col in df.columns:\n",
    "    # Get basic information\n",
    "    dtype = df[col].dtype\n",
    "    unique_count = df[col].nunique()\n",
    "    \n",
    "    # Initialize extra information\n",
    "    value_range = 'N/A' # Default for non-numeric columns\n",
    "    mean_val = 'N/A'\n",
    "    std_dev = 'N/A'\n",
    "\n",
    "    # Infer feature type and calculate range\n",
    "    if col == 'COHORT':\n",
    "        feature_type = 'Target'\n",
    "    elif col == 'PATNO':\n",
    "        feature_type = 'Identifier'\n",
    "    elif dtype == 'object':\n",
    "        feature_type = 'Categorical-String'\n",
    "    elif pd.api.types.is_numeric_dtype(dtype):\n",
    "        # If the column has no valid values, min/max will raise an error, so handle it\n",
    "        if not df[col].dropna().empty:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            value_range = f\"{min_val:.2f} - {max_val:.2f}\"\n",
    "            # Calculate mean and standard deviation\n",
    "            mean_val = f\"{df[col].mean():.2f}\"\n",
    "            std_dev = f\"{df[col].std():.2f}\"\n",
    "        else:\n",
    "            value_range = 'All NaN'\n",
    "            mean_val = 'All NaN'\n",
    "            std_dev = 'All NaN'\n",
    "\n",
    "        # Further distinguish numeric types\n",
    "        if unique_count == 2:\n",
    "            feature_type = 'Numerical-Binary'\n",
    "        elif unique_count < 25:\n",
    "            feature_type = 'Numerical-Categorical'\n",
    "        else:\n",
    "            feature_type = 'Numerical-Continuous'\n",
    "    else:\n",
    "        feature_type = 'Other'\n",
    "\n",
    "    # Get the expected range\n",
    "    expected_range_str = expected_ranges.get(col, {}).get('range_str', 'N/A')\n",
    "        \n",
    "    feature_analysis.append({\n",
    "        'Feature': col,\n",
    "        'Dtype': dtype,\n",
    "        'Unique Values': unique_count,\n",
    "        'Value Range': value_range,\n",
    "        'Expected Range': expected_range_str,\n",
    "        'Mean': mean_val,\n",
    "        'Std Dev': std_dev,\n",
    "        'Inferred Type': feature_type\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display as a table\n",
    "analysis_df = pd.DataFrame(feature_analysis)\n",
    "\n",
    "# Reorder the columns\n",
    "desired_order = [\n",
    "    'Feature', \n",
    "    'Inferred Type',\n",
    "    'Dtype', \n",
    "    'Unique Values', \n",
    "    'Value Range', \n",
    "    'Expected Range',\n",
    "    'Mean', \n",
    "    'Std Dev'\n",
    "]\n",
    "analysis_df = analysis_df[desired_order]\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # Display all rows\n",
    "    display(analysis_df)\n",
    "\n",
    "print(\"\\nAnalysis: The table above summarizes the data type, inferred purpose, and value range for each feature.\")\n",
    "print(\"By comparing 'Value Range' and 'Expected Range', we can quickly identify potential outliers or distributions that do not match expectations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Missing Value Analysis ---\n",
    "print(\"--- Missing Value Analysis ---\")\n",
    "\n",
    "# Calculate the number and percentage of missing values for each feature\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Create a new DataFrame with missing value information, sorted by percentage descending\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values(by='Missing Percentage', ascending=False)\n",
    "\n",
    "# Filter out features with no missing values\n",
    "missing_info = missing_info[missing_info['Missing Count'] >= 0]\n",
    "\n",
    "print(f\"Missing Value Status\\n\")\n",
    "display(missing_info)\n",
    "\n",
    "# Visualize the percentage of missing values\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x=missing_info.index, y=missing_info['Missing Percentage'], hue=missing_info.index, palette='viridis', legend=False)\n",
    "plt.title('Percentage of Missing Values in Each Feature', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Missing Percentage (%)', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Target Variable (COHORT) Distribution Analysis ---\n",
    "print(\"--- Target Variable COHORT Distribution Analysis ---\")\n",
    "\n",
    "# Calculate the count and percentage of each category\n",
    "cohort_counts = df['COHORT'].value_counts()\n",
    "cohort_percentages = df['COHORT'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"COHORT Category Distribution:\")\n",
    "for cohort_id, count in cohort_counts.items():\n",
    "    print(f\"  - Category {cohort_id}: {count} records ({cohort_percentages[cohort_id]:.2f}%)\")\n",
    "\n",
    "# Visualize the category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='COHORT', data=df, hue='COHORT', palette='pastel', legend=False)\n",
    "plt.title('Distribution of Diagnostic Groups (COHORT)', fontsize=16)\n",
    "plt.xlabel('COHORT Label', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "\n",
    "# Display percentage on each bar\n",
    "for p in ax.patches:\n",
    "    percentage = f'{100 * p.get_height() / len(df):.2f}%'\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Object Type Feature Analysis ---\n",
    "print(\"--- Object Type Feature Analysis ---\")\n",
    "\n",
    "# Analyze EVENT_ID\n",
    "print(\"\\n--- Analyzing EVENT_ID ---\")\n",
    "event_id_counts = df['EVENT_ID'].value_counts()\n",
    "print(f\"EVENT_ID has {len(event_id_counts)} unique values.\")\n",
    "print(\"Top 10 most common EVENT_ID values:\")\n",
    "print(event_id_counts.head(10))\n",
    "\n",
    "# Visualize EVENT_ID distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=event_id_counts.head(15).index, y=event_id_counts.head(15).values, hue=event_id_counts.head(15).index, palette='coolwarm', legend=False)\n",
    "plt.title('Top 15 Most Common EVENT_IDs (Visits)', fontsize=16)\n",
    "plt.xlabel('EVENT_ID', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.1. View All Unique Values of PCTL_BNT ---\n",
    "print(\"--- All Unique Values in 'PCTL_BNT' Column ---\")\n",
    "\n",
    "# Use .dropna().unique() to get all non-null unique values\n",
    "pctl_bnt_all_unique_values = df['PCTL_BNT'].dropna().unique()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Found {len(pctl_bnt_all_unique_values)} unique non-null values:\")\n",
    "for value in pctl_bnt_all_unique_values:\n",
    "    print(f\"- {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6.2. Load and Verify PCTL_BNT Cleaned Data ---\n",
    "# Note: We assume the clean_pctl_bnt.py script has been run, generating 'raw_data_2.0.csv'\n",
    "data_path_v2 = '../data/raw_data_2.0.csv'\n",
    "\n",
    "print(f\"--- Loading v2.0 (PCTL_BNT cleaned) data ---\")\n",
    "print(f\"File path: '{data_path_v2}'\")\n",
    "# Load into a new DataFrame for comparison\n",
    "df_v2 = pd.read_csv(data_path_v2)\n",
    "print(\"\\nv2.0 data loaded successfully!\")\n",
    "\n",
    "pctl_bnt_all_unique_values = df_v2['PCTL_BNT'].dropna().unique()\n",
    "# Additional verification of the PCTL_BNT column\n",
    "print(f\"Found {len(pctl_bnt_all_unique_values)} unique non-null values:\")\n",
    "for value in pctl_bnt_all_unique_values:\n",
    "    print(f\"- {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Outlier Analysis (IQR Method) ---\n",
    "print(\"--- Outlier Detection using IQR Method ---\")\n",
    "\n",
    "# === 1. Load the list of numerical features from analysis_features.json ===\n",
    "with open('../data/analysis_features.json', 'r', encoding='utf-8') as f:\n",
    "    analysis_features = json.load(f)\n",
    "\n",
    "numeric_features_for_outliers = analysis_features.get('numerical_features', [])\n",
    "\n",
    "# === 2. Ensure these features exist in df_v2 ===\n",
    "numeric_features_for_outliers = [\n",
    "    f for f in numeric_features_for_outliers if f in df_v2.columns\n",
    "]\n",
    "\n",
    "# === 3. Remove ID and target columns (to prevent interference) ===\n",
    "numeric_features_for_outliers = [\n",
    "    col for col in numeric_features_for_outliers\n",
    "    if col not in ['PATNO', 'COHORT']\n",
    "]\n",
    "\n",
    "# === 4. Create a results list ===\n",
    "outlier_summary = []\n",
    "\n",
    "# === 5. Iterate through all numerical features and apply the IQR method ===\n",
    "for col in numeric_features_for_outliers:\n",
    "    feature_data = df_v2[col].dropna()  # Remove missing values\n",
    "    \n",
    "    # If the feature has no data, skip it\n",
    "    if feature_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Calculate Q1, Q3, IQR\n",
    "    Q1 = feature_data.quantile(0.25)\n",
    "    Q3 = feature_data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier boundaries (using 1.5 * IQR is common)\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = feature_data[(feature_data < lower_bound) | (feature_data > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    total_count = len(feature_data)\n",
    "    outlier_ratio = outlier_count / total_count if total_count > 0 else np.nan\n",
    "    \n",
    "    # Store the results\n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'Q1': round(Q1, 2),\n",
    "        'Q3': round(Q3, 2),\n",
    "        'IQR': round(IQR, 2),\n",
    "        'Lower Bound': round(lower_bound, 2),\n",
    "        'Upper Bound': round(upper_bound, 2),\n",
    "        'Outlier Count': outlier_count,\n",
    "        'Total Count': total_count,\n",
    "        'Outlier Ratio (%)': round(outlier_ratio * 100, 2)\n",
    "    })\n",
    "\n",
    "# === 6. Convert to DataFrame and display ===\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df.sort_values(by='Outlier Ratio (%)', ascending=False)\n",
    "outlier_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = (\n",
    "    df_v2.select_dtypes(include=np.number)\n",
    "      .drop(columns=['PATNO', 'COHORT'])\n",
    "      .columns\n",
    ")\n",
    "\n",
    "n_features = len(numeric_features)\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 3.5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    sns.histplot(df_v2[feature].dropna(), kde=False, ax=axes[i], bins=30)\n",
    "    axes[i].set_yscale('log')\n",
    "    axes[i].set_title(f'Distribution of {feature}', fontsize=12)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "fig.suptitle('Distribution of All Numerical Features', fontsize=20, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Boxplots for All Numerical Features by COHORT ---\n",
    "print(\"--- Plotting Boxplots for All Numerical Features (Grouped by COHORT) ---\")\n",
    "\n",
    "# 1️⃣ Select numerical features (consistent with previous steps)\n",
    "numeric_features = (\n",
    "    df_v2.select_dtypes(include=np.number)\n",
    "      .drop(columns=['PATNO', 'COHORT'])\n",
    "      .columns\n",
    ")\n",
    "\n",
    "n_features = len(numeric_features)\n",
    "print(f\"Detected {n_features} numerical features.\")\n",
    "\n",
    "# 2️⃣ Layout parameters\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "# 3️⃣ Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 4️⃣ Iterate through each feature and plot a boxplot\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    sns.boxplot(\n",
    "        x='COHORT', \n",
    "        y=feature, \n",
    "        data=df_v2, \n",
    "        ax=axes[i],\n",
    "        hue='COHORT', \n",
    "        palette='Set2',\n",
    "        legend=False\n",
    "    )\n",
    "    axes[i].set_title(f'{feature} by COHORT', fontsize=12)\n",
    "    axes[i].set_xlabel('COHORT', fontsize=10)\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "# Remove any extra empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# 5️⃣ Main title and layout adjustment\n",
    "fig.suptitle('Distribution of All Numerical Features by COHORT', fontsize=20, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Feature Correlation Heatmap ---\n",
    "print(\"--- Feature Correlation Analysis ---\")\n",
    "\n",
    "# Calculate the correlation matrix for numerical features\n",
    "# We will randomly select a subset (e.g., 15) of the numeric_features for visualization,\n",
    "# because a heatmap with over 50 features would be too dense to read.\n",
    "if len(numeric_features) > 15:\n",
    "    corr_features = np.random.choice(numeric_features, 15, replace=False)\n",
    "else:\n",
    "    corr_features = numeric_features\n",
    "\n",
    "print(f\"Calculating correlation matrix for the following {len(corr_features)} random features:\")\n",
    "print(corr_features)\n",
    "\n",
    "correlation_matrix = df_v2[corr_features].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Correlation Matrix of Selected Numerical Features', fontsize=18)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
